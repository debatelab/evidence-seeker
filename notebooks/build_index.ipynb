{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build and upload index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evidence_seeker.retrieval import build_index, RetrievalConfig\n",
    "import pathlib\n",
    "\n",
    "document_input_dir = \"../TMP/APUZ_PART/corpus\"\n",
    "# List all pdf files (we do not need to index the metadata files)\n",
    "pdf_files = [\n",
    "    str(p) for p in pathlib.Path(document_input_dir).rglob(\"*\") \n",
    "    if p.is_file() and (p.name.endswith(\".pdf\") or p.name.endswith(\".PDF\"))\n",
    "]\n",
    "\n",
    "\n",
    "config = RetrievalConfig(\n",
    "    ###### MODEL CONFIGURATION ##############\n",
    "    ### Local model (via Huggingface API) ###\n",
    "    embed_backend_type=\"huggingface\",\n",
    "    embed_model_name=\"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\",   \n",
    "    ### Huggingface inference API ###########\n",
    "    # embed_backend_type=\"huggingface_inference_api\",\n",
    "    # embed_model_name=\"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\",\n",
    "    # embed_base_url=\"https://router.huggingface.co/hf-inference/models/sentence-transformers/paraphrase-multilingual-mpnet-base-v2\",\n",
    "    # api_key_name=\"hf_debatelab_inference_provider\",\n",
    "    # bill_to=\"DebateLabKIT\",\n",
    "    ####### END MODEL CONFIGURATIAN #########\n",
    "    #hub_key_name=\"hf_evse_data\",\n",
    "    document_input_dir=document_input_dir,\n",
    "    document_input_files=pdf_files,\n",
    "    index_persist_path=\"../TMP/APUZ_PART/storage\",\n",
    "    # uncomment the following line to upload the index to the HF hub\n",
    "    #index_hub_path = \"DebateLabKIT/apuz-index-es\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load meta data from yaml files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-06-17 11:32:37.135\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m12\u001b[0m - \u001b[1mMetadata directory: ../TMP/APUZ_PART/corpus\u001b[0m\n",
      "\u001b[32m2025-06-17 11:32:37.136\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m15\u001b[0m - \u001b[1mLoading metadata from ../TMP/APUZ_PART/corpus/ahrens_pragmatischer_2024.yaml\u001b[0m\n",
      "\u001b[32m2025-06-17 11:32:37.138\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m15\u001b[0m - \u001b[1mLoading metadata from ../TMP/APUZ_PART/corpus/alexander_stabil_2024.yaml\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 11 documents with metadata from ../TMP/APUZ_PART/corpus\n"
     ]
    }
   ],
   "source": [
    "from loguru import logger\n",
    "import pathlib\n",
    "import yaml\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "from pprint import pprint\n",
    "from typing import Dict\n",
    "import os\n",
    "#import chardet \n",
    "\n",
    "metadata_dict = {}\n",
    "metadata_dir = config.document_input_dir or os.path.dirname(config.document_input_files[0])\n",
    "logger.info(f\"Metadata directory: {metadata_dir}\")\n",
    "# load and parse all yaml files in metadata_dir\n",
    "for filepath in pathlib.Path(metadata_dir).rglob(\"*.yaml\"):\n",
    "    logger.info(f\"Loading metadata from {filepath}\")\n",
    "\n",
    "    # if the corresponding pdf file does not exist, skip this metadata file\n",
    "    pdf_file = os.path.join(metadata_dir, filepath.stem + \".pdf\")\n",
    "    if not pathlib.Path(pdf_file).is_file():\n",
    "        logger.warning(f\"PDF file {pdf_file} does not exist, skipping metadata file {filepath}.\")\n",
    "        continue\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = yaml.safe_load(f)\n",
    "        if (\n",
    "            \"file\" in data \n",
    "            and \"author\" in data \n",
    "            and \"url\" in data\n",
    "            and \"title\" in data\n",
    "        ):\n",
    "            filename = data[\"file\"]\n",
    "            metadata_dict[filename] = {\n",
    "                \"author\": data[\"author\"],\n",
    "                \"title\": data[\"title\"], \n",
    "                \"url\": data[\"url\"],\n",
    "                \"year\": data.get(\"year\", None),  # optional field\n",
    "                \"month\": data.get(\"month\", None),  # optional field\n",
    "                # added by hand (not correctly in the yaml files/bibtex file)\n",
    "                \"journal\": \"Aus Politik und Zeitgeschichte (APuZ)\",\n",
    "            }\n",
    "        else:\n",
    "            logger.warning(f\"Invalid metadata in {filepath}.\")\n",
    "        \n",
    "\n",
    "def document_file_metadata(filename: str) -> Dict: \n",
    "    meta = metadata_dict.get(pathlib.Path(filename).name, {})\n",
    "    if not meta:\n",
    "        logger.warning(f\"No metadata found for file: {filename}\")\n",
    "    return meta\n",
    "\n",
    "\n",
    "# for debugging purposes, print the metadata dictionary\n",
    "reader = SimpleDirectoryReader(\n",
    "    input_files=pdf_files,\n",
    "    file_metadata=document_file_metadata,\n",
    ")\n",
    "docs = reader.load_data()\n",
    "print(f\"Loaded {len(docs)} documents with metadata from {metadata_dir}\")\n",
    "#for doc in docs:\n",
    "#    print(doc.metadata)\n",
    "\n",
    "#pprint(docs[0].metadata)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-06-17 11:35:51.740\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mevidence_seeker.retrieval.base\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m383\u001b[0m - \u001b[33m\u001b[1mNo API token provided for embedding model.\u001b[0m\n",
      "\u001b[32m2025-06-17 11:35:54.502\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m11\u001b[0m - \u001b[1mBuilding index in '/home/basti/Nextcloud/Documents/mindmaps/mind/projects/kideku/code/evidence-seeker/TMP/APUZ_PART/storage/index' ...\u001b[0m\n",
      "\u001b[32m2025-06-17 11:35:54.503\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mevidence_seeker.retrieval.base\u001b[0m:\u001b[36mbuild_index\u001b[0m:\u001b[36m414\u001b[0m - \u001b[33m\u001b[1mBoth document_input_dir and document_input_files provided. Using document_input_files.\u001b[0m\n",
      "\u001b[32m2025-06-17 11:35:54.503\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mevidence_seeker.retrieval.base\u001b[0m:\u001b[36mbuild_index\u001b[0m:\u001b[36m423\u001b[0m - \u001b[34m\u001b[1mReading documents from ['../TMP/APUZ_PART/corpus/alexander_stabil_2024.pdf', '../TMP/APUZ_PART/corpus/ahrens_pragmatischer_2024.pdf']\u001b[0m\n",
      "\u001b[32m2025-06-17 11:35:54.503\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mevidence_seeker.retrieval.base\u001b[0m:\u001b[36mbuild_index\u001b[0m:\u001b[36m428\u001b[0m - \u001b[1mBuilding document index...\u001b[0m\n",
      "\u001b[32m2025-06-17 11:35:54.985\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mevidence_seeker.retrieval.base\u001b[0m:\u001b[36mbuild_index\u001b[0m:\u001b[36m436\u001b[0m - \u001b[34m\u001b[1mParsing nodes...\u001b[0m\n",
      "\u001b[32m2025-06-17 11:35:55.104\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mevidence_seeker.retrieval.base\u001b[0m:\u001b[36mbuild_index\u001b[0m:\u001b[36m443\u001b[0m - \u001b[34m\u001b[1mCreating VectorStoreIndex with embeddings...\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "608ba5d3827d4a1d8406fc67edfa44a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/465 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-06-17 11:36:24.082\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mevidence_seeker.retrieval.base\u001b[0m:\u001b[36mbuild_index\u001b[0m:\u001b[36m453\u001b[0m - \u001b[34m\u001b[1mPersisting index to /home/basti/Nextcloud/Documents/mindmaps/mind/projects/kideku/code/evidence-seeker/TMP/APUZ_PART/storage/index\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from evidence_seeker.retrieval.base import IndexBuilder\n",
    "from os import path\n",
    "\n",
    "index_builder = IndexBuilder(\n",
    "    config=config,\n",
    "    env_file=\"../.env\",  # path to your .env file\n",
    ")\n",
    "\n",
    "index_builder.build_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "517a77ce9f224b90bd672b14cdcea565",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 2 LFS files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2780337b3e94405b305862e41ee3c10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "default__vector_store.json:   0%|          | 0.00/650M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e67c5d79b4243c0b6c17798aa707539",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "docstore.json:   0%|          | 0.00/208M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/DebateLabKIT/apuz-index-es/commit/799dee54e009b532410c4765368ee1bcbdade16b', commit_message='Upload folder using huggingface_hub', commit_description='', oid='799dee54e009b532410c4765368ee1bcbdade16b', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/DebateLabKIT/apuz-index-es', endpoint='https://huggingface.co', repo_type='dataset', repo_id='DebateLabKIT/apuz-index-es'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# explicitly upload the index to the HuggingFace hub \n",
    "# (can also be done via the `build_index` function)\n",
    "from evidence_seeker.retrieval.base import INDEX_PATH_IN_REPO\n",
    "\n",
    "HfApi = huggingface_hub.HfApi(token=hub_token)\n",
    "HfApi.upload_folder(\n",
    "    repo_id=\"DebateLabKIT/apuz-index-es\",\n",
    "    folder_path=config.index_persist_path,\n",
    "    #path_in_repo=INDEX_PATH_IN_REPO,\n",
    "    repo_type=\"dataset\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "evse-dev_env.py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
