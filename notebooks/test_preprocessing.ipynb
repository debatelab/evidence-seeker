{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initiating observation via Phoenix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🌍 To view the Phoenix app in your browser, visit http://localhost:6006/\n",
      "📖 For more information on how to use Phoenix, check out https://docs.arize.com/phoenix\n"
     ]
    }
   ],
   "source": [
    "# %pip pip install arize-phoenix\n",
    "# %pip install llama-index-callbacks-arize-phoenix\n",
    "\n",
    "# observability\n",
    "import phoenix as px\n",
    "px.launch_app()\n",
    "\n",
    "import llama_index.core\n",
    "llama_index.core.set_global_handler(\"arize_phoenix\", endpoint=\"http://localhost:6006/v1/traces\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing workflows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'NoneType'>\n",
      "<class 'evidence_seeker.preprocessing.workflows.ListAscriptiveClaimsEvent'>\n",
      "<class 'llama_index.core.workflow.events.StopEvent'>\n",
      "<class 'evidence_seeker.preprocessing.workflows.ListDescriptiveClaimsEvent'>\n",
      "<class 'evidence_seeker.preprocessing.workflows.NegateClaimEvent'>\n",
      "<class 'evidence_seeker.preprocessing.workflows.StartedNegatingClaims'>\n",
      "<class 'evidence_seeker.preprocessing.workflows.NegateClaimEvent'>\n",
      "<class 'evidence_seeker.preprocessing.workflows.StartedNegatingClaims'>\n",
      "<class 'evidence_seeker.preprocessing.workflows.NegateClaimEvent'>\n",
      "<class 'evidence_seeker.preprocessing.workflows.CollectClarifiedClaimsEvent'>\n",
      "<class 'evidence_seeker.preprocessing.workflows.ListNormativeClaimsEvent'>\n",
      "<class 'evidence_seeker.preprocessing.workflows.NormativeAnalysisEvent'>\n",
      "<class 'evidence_seeker.preprocessing.workflows.DescriptiveAnalysisEvent'>\n",
      "<class 'evidence_seeker.preprocessing.workflows.AscriptiveAnalysisEvent'>\n",
      "../TMP/PreprocessingWorkflow.html\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "from llama_index.utils.workflow import draw_all_possible_flows\n",
    "\n",
    "from evidence_seeker.preprocessing.workflows import (\n",
    "    PreprocessingWorkflow,\n",
    ")\n",
    "\n",
    "draw_all_possible_flows(\n",
    "    PreprocessingWorkflow, filename=\"../TMP/PreprocessingWorkflow.html\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save default config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7841"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pathlib\n",
    "import yaml\n",
    "\n",
    "from evidence_seeker.preprocessing import ClaimPreprocessingConfig\n",
    "\n",
    "configfile = pathlib.Path(\"../configs/preprocessing_config_default.yaml\")\n",
    "\n",
    "default_config = ClaimPreprocessingConfig()\n",
    "default_config_dict = default_config.model_dump()\n",
    "\n",
    "#configfile.write_text(yaml.dump(default_config_dict))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running ClaimPreprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n",
      "\u001b[32m2024-12-13 10:44:06.582\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mevidence_seeker.backend\u001b[0m:\u001b[36mget_openai_llm\u001b[0m:\u001b[36m109\u001b[0m - \u001b[34m\u001b[1mFetching api key via env var: HF_TOKEN_EVIDENCE_SEEKER\u001b[0m\n",
      "\u001b[32m2024-12-13 10:44:06.584\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mevidence_seeker.backend\u001b[0m:\u001b[36mget_openai_llm\u001b[0m:\u001b[36m117\u001b[0m - \u001b[34m\u001b[1mInstantiating OpenAILike model (model: meta-llama/Llama-3.3-70B-Instruct,base_url: https://kbg7e8kdjsvkg38j.us-east-1.aws.endpoints.huggingface.cloud/v1/).\u001b[0m\n",
      "\u001b[32m2024-12-13 10:44:06.586\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mevidence_seeker.preprocessing.workflows\u001b[0m:\u001b[36mascriptive_analysis\u001b[0m:\u001b[36m168\u001b[0m - \u001b[34m\u001b[1mAnalysing ascriptive aspects of claim.\u001b[0m\n",
      "\u001b[32m2024-12-13 10:44:06.629\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mevidence_seeker.preprocessing.workflows\u001b[0m:\u001b[36mdescriptive_analysis\u001b[0m:\u001b[36m119\u001b[0m - \u001b[34m\u001b[1mAnalysing descriptive aspects of claim.\u001b[0m\n",
      "\u001b[32m2024-12-13 10:44:06.633\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mevidence_seeker.preprocessing.workflows\u001b[0m:\u001b[36mnormative_analysis\u001b[0m:\u001b[36m217\u001b[0m - \u001b[34m\u001b[1mAnalysing normative aspects of claim.\u001b[0m\n",
      "\u001b[32m2024-12-13 10:44:30.136\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mevidence_seeker.preprocessing.workflows\u001b[0m:\u001b[36mnegate_claim\u001b[0m:\u001b[36m266\u001b[0m - \u001b[34m\u001b[1mNegating claim.\u001b[0m\n",
      "\u001b[32m2024-12-13 10:44:30.141\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mevidence_seeker.preprocessing.workflows\u001b[0m:\u001b[36mnegate_claim\u001b[0m:\u001b[36m266\u001b[0m - \u001b[34m\u001b[1mNegating claim.\u001b[0m\n",
      "\u001b[32m2024-12-13 10:44:30.147\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mevidence_seeker.preprocessing.workflows\u001b[0m:\u001b[36mnegate_claim\u001b[0m:\u001b[36m266\u001b[0m - \u001b[34m\u001b[1mNegating claim.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "from evidence_seeker.preprocessing import ClaimPreprocessor, ClaimPreprocessingConfig\n",
    "import pathlib\n",
    "\n",
    "#config_file = pathlib.Path(\"../configs/confirmation_analysis_config_default.yaml\")\n",
    "#confirmation_analyzer = ConfirmationAnalyzer.from_config_file(config_file)\n",
    "\n",
    "\n",
    "#config_file = \"../configs/preprocessing_config_default.yaml\" \n",
    "#config = ClaimPreprocessingConfig.from_config_file(config_file)\n",
    "\n",
    "config = ClaimPreprocessingConfig()\n",
    "preprocessor = ClaimPreprocessor(config=config)\n",
    "\n",
    "clarified_claims = await preprocessor(claim=\"Climate change is the biggest challenge for human kind.\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CheckedClaim(text='Climate change is more significant than other global issues', negation='Climate change is not more significant than other global issues.', uid='90bd5c23-8172-436e-9ec4-9624fd99cb9d', statement_type=<StatementType.NORMATIVE: 'normative'>, n_evidence=None, average_confirmation=None, evidential_uncertainty=None, verbalized_confirmation=None, documents=None, confirmation_by_document=None, metadata={}),\n",
      " CheckedClaim(text='The impact of climate change on human societies is the most severe among all challenges', negation='The impact of climate change on human societies is not the most severe among all challenges.', uid='08bf3098-4c40-4b67-8dbd-d24464bafa7f', statement_type=<StatementType.NORMATIVE: 'normative'>, n_evidence=None, average_confirmation=None, evidential_uncertainty=None, verbalized_confirmation=None, documents=None, confirmation_by_document=None, metadata={}),\n",
      " CheckedClaim(text='Addressing climate change should be prioritized over other challenges facing humanity', negation='Addressing climate change should not be prioritized over other challenges facing humanity.', uid='ab7c1daa-ea9a-4d1b-a9c2-e322b756da5a', statement_type=<StatementType.NORMATIVE: 'normative'>, n_evidence=None, average_confirmation=None, evidential_uncertainty=None, verbalized_confirmation=None, documents=None, confirmation_by_document=None, metadata={})]\n"
     ]
    }
   ],
   "source": [
    "pprint(clarified_claims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "evidence-seeker",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
