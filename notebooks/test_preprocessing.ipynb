{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initiating observation via Phoenix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŒ To view the Phoenix app in your browser, visit http://localhost:6006/\n",
      "ðŸ“– For more information on how to use Phoenix, check out https://docs.arize.com/phoenix\n"
     ]
    }
   ],
   "source": [
    "# %pip pip install arize-phoenix\n",
    "# %pip install llama-index-callbacks-arize-phoenix\n",
    "\n",
    "# observability\n",
    "import phoenix as px\n",
    "px.launch_app()\n",
    "\n",
    "import llama_index.core\n",
    "llama_index.core.set_global_handler(\"arize_phoenix\", endpoint=\"http://localhost:6006/v1/traces\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing workflows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'NoneType'>\n",
      "<class 'evidence_seeker.preprocessing.workflows.ListAscriptiveClaimsEvent'>\n",
      "<class 'llama_index.core.workflow.events.StopEvent'>\n",
      "<class 'evidence_seeker.preprocessing.workflows.ListDescriptiveClaimsEvent'>\n",
      "<class 'evidence_seeker.preprocessing.workflows.NegateClaimEvent'>\n",
      "<class 'evidence_seeker.preprocessing.workflows.StartedNegatingClaims'>\n",
      "<class 'evidence_seeker.preprocessing.workflows.NegateClaimEvent'>\n",
      "<class 'evidence_seeker.preprocessing.workflows.StartedNegatingClaims'>\n",
      "<class 'evidence_seeker.preprocessing.workflows.NegateClaimEvent'>\n",
      "<class 'evidence_seeker.preprocessing.workflows.CollectClarifiedClaimsEvent'>\n",
      "<class 'evidence_seeker.preprocessing.workflows.ListNormativeClaimsEvent'>\n",
      "<class 'evidence_seeker.preprocessing.workflows.NormativeAnalysisEvent'>\n",
      "<class 'evidence_seeker.preprocessing.workflows.DescriptiveAnalysisEvent'>\n",
      "<class 'evidence_seeker.preprocessing.workflows.AscriptiveAnalysisEvent'>\n",
      "../TMP/PreprocessingSeparateListingsWorkflow.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1733932923.593460  563591 fork_posix.cc:75] Other threads are currently calling into gRPC, skipping fork() handlers\n",
      "I0000 00:00:1733932923.611199  563591 fork_posix.cc:75] Other threads are currently calling into gRPC, skipping fork() handlers\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "from llama_index.utils.workflow import draw_all_possible_flows\n",
    "\n",
    "from evidence_seeker.preprocessing.workflows import (\n",
    "    PreprocessingWorkflow,\n",
    ")\n",
    "# from evidence_seeker.preprocessing.simple_preprocessing_workflow import SimplePreprocessingWorkflow\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "draw_all_possible_flows(\n",
    "    PreprocessingWorkflow, filename=\"../TMP/PreprocessingSeparateListingsWorkflow.html\"\n",
    ")\n",
    "\n",
    "#draw_all_possible_flows(\n",
    "#    SimplePreprocessingWorkflow, filename=\"../tmp/SimplePreprocessingWorkflow.html\"\n",
    "#)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running Workflows directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from pprint import pprint\n",
    "# \n",
    "# from evidence_seeker.preprocessing.preprocessing_separate_listings_workflow import (\n",
    "#     PreprocessingSeparateListingsWorkflow,\n",
    "# )\n",
    "# \n",
    "# \n",
    "# # config_file = \"../config.yaml\" \n",
    "# \n",
    "# config_file = \"../configs/preprocessing_separate_listings_config.yaml\" \n",
    "# pw = PreprocessingSeparateListingsWorkflow(config_file=config_file)\n",
    "# \n",
    "# #config_file = \"../configs/simple_preprocessing_config.yaml\" \n",
    "# #pw = SimplePreprocessingWorkflow(config_file=config_file)\n",
    "# \n",
    "# examples = [\n",
    "#     \"Climate change is the biggest challenge for human kind.\",\n",
    "#     \"Climate change is getting ever and ever faster.\",\n",
    "#     \"The rate of climate change is increasing over time.\",\n",
    "# ]\n",
    "# result = await pw.run(claim=examples[2])\n",
    "# pprint(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save default config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7669"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pathlib\n",
    "import yaml\n",
    "\n",
    "from evidence_seeker.preprocessing import ClaimPreprocessingConfig\n",
    "\n",
    "configfile = pathlib.Path(\"../configs/preprocessing_config_default.yaml\")\n",
    "\n",
    "default_config = ClaimPreprocessingConfig()\n",
    "default_config_dict = default_config.model_dump()\n",
    "\n",
    "configfile.write_text(yaml.dump(default_config_dict))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running ClaimPreprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-12-11 17:14:42.337\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mevidence_seeker.backend\u001b[0m:\u001b[36mget_openai_llm\u001b[0m:\u001b[36m117\u001b[0m - \u001b[34m\u001b[1mInstantiating OpenAILike model (model: meta-llama/Llama-3.1-70B-Instruct,base_url: https://huggingface.co/api/integrations/dgx/v1).\u001b[0m\n",
      "\u001b[32m2024-12-11 17:14:42.340\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mevidence_seeker.preprocessing.workflows\u001b[0m:\u001b[36mascriptive_analysis\u001b[0m:\u001b[36m176\u001b[0m - \u001b[34m\u001b[1mAnalysing ascriptive aspects of claim.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-12-11 17:14:42.387\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mevidence_seeker.preprocessing.workflows\u001b[0m:\u001b[36mdescriptive_analysis\u001b[0m:\u001b[36m127\u001b[0m - \u001b[34m\u001b[1mAnalysing descriptive aspects of claim.\u001b[0m\n",
      "\u001b[32m2024-12-11 17:14:42.392\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mevidence_seeker.preprocessing.workflows\u001b[0m:\u001b[36mnormative_analysis\u001b[0m:\u001b[36m225\u001b[0m - \u001b[34m\u001b[1mAnalysing normative aspects of claim.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching api key via env var: HF_TOKEN_EVIDENCE_SEEKER\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-12-11 17:14:52.252\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mevidence_seeker.preprocessing.workflows\u001b[0m:\u001b[36mnegate_claim\u001b[0m:\u001b[36m274\u001b[0m - \u001b[34m\u001b[1mNegating claim.\u001b[0m\n",
      "\u001b[32m2024-12-11 17:14:52.258\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mevidence_seeker.preprocessing.workflows\u001b[0m:\u001b[36mnegate_claim\u001b[0m:\u001b[36m274\u001b[0m - \u001b[34m\u001b[1mNegating claim.\u001b[0m\n",
      "\u001b[32m2024-12-11 17:14:52.576\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mevidence_seeker.preprocessing.workflows\u001b[0m:\u001b[36mnegate_claim\u001b[0m:\u001b[36m274\u001b[0m - \u001b[34m\u001b[1mNegating claim.\u001b[0m\n",
      "\u001b[32m2024-12-11 17:14:52.582\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mevidence_seeker.preprocessing.workflows\u001b[0m:\u001b[36mnegate_claim\u001b[0m:\u001b[36m274\u001b[0m - \u001b[34m\u001b[1mNegating claim.\u001b[0m\n",
      "\u001b[32m2024-12-11 17:14:52.588\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mevidence_seeker.preprocessing.workflows\u001b[0m:\u001b[36mnegate_claim\u001b[0m:\u001b[36m274\u001b[0m - \u001b[34m\u001b[1mNegating claim.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "from pprint import pprint\n",
    "\n",
    "from evidence_seeker.preprocessing import ClaimPreprocessor, ClaimPreprocessingConfig\n",
    "\n",
    "#config_file = \"../configs/preprocessing_separate_listings_config.yaml\" \n",
    "#config_file = \"../configs/simple_preprocessing_config.yaml\" \n",
    "config = ClaimPreprocessingConfig(**yaml.safe_load(configfile.read_text()))\n",
    "#config = ClaimPreprocessingConfig()\n",
    "\n",
    "# Note: Use preprocessor workflow is currently hardset in `ClaimPreprocessor`\n",
    "preprocessor = ClaimPreprocessor(config=config)\n",
    "clarified_claims = await preprocessor(claim=\"Climate change is the biggest challenge for human kind.\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CheckedClaim(text='There are other significant challenges facing human kind', negation='There are no other significant challenges facing humanity.', uid='371dd3fe-001d-4a2a-a689-730610808104', n_evidence=None, average_confirmation=None, evidential_uncertainty=None, verbalized_confirmation=None, documents=None, confirmation_by_document=None, metadata={'statement_type': 'descriptive'}),\n",
      " CheckedClaim(text='Climate change has significant impacts on human societies, ecosystems, and economies', negation='Climate change has no significant impacts on human societies, ecosystems, and economies.', uid='b20bff00-83a9-430d-a5c9-c55c3f59b644', n_evidence=None, average_confirmation=None, evidential_uncertainty=None, verbalized_confirmation=None, documents=None, confirmation_by_document=None, metadata={'statement_type': 'descriptive'}),\n",
      " CheckedClaim(text='Addressing climate change is of utmost importance for humanity', negation='Addressing climate change is not of utmost importance for humanity.', uid='74e23b97-f925-47a5-9850-5561ef280a3d', n_evidence=None, average_confirmation=None, evidential_uncertainty=None, verbalized_confirmation=None, documents=None, confirmation_by_document=None, metadata={'statement_type': 'normative'}),\n",
      " CheckedClaim(text='Climate change is a significant threat to human well-being or survival', negation='Climate change is not a significant threat to human well-being or survival.', uid='d6b4167c-d5e0-4534-81d2-49653f2a57e5', n_evidence=None, average_confirmation=None, evidential_uncertainty=None, verbalized_confirmation=None, documents=None, confirmation_by_document=None, metadata={'statement_type': 'normative'}),\n",
      " CheckedClaim(text='Climate change has a greater potential to negatively impact human societies and ecosystems than other challenges', negation='Climate change does not have a greater potential to negatively impact human societies and ecosystems than other challenges.', uid='edfe927f-6318-4f03-9e70-39d922e8c724', n_evidence=None, average_confirmation=None, evidential_uncertainty=None, verbalized_confirmation=None, documents=None, confirmation_by_document=None, metadata={'statement_type': 'normative'})]\n"
     ]
    }
   ],
   "source": [
    "pprint(clarified_claims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "evidence-seeker",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
